# tensorQTL Test Suite

This directory contains a comprehensive test suite for tensorQTL, covering all major functionality including unit tests, integration tests, and performance benchmarks.

## Test Structure

### Core Tests
- **`test_core.py`** - Tests for core utilities (Residualizer, statistical functions, data types)
- **`test_genotypeio.py`** - Tests for genotype/phenotype I/O operations
- **`test_cis.py`** - Tests for cis-QTL mapping functionality
- **`test_trans.py`** - Tests for trans-QTL mapping functionality
- **`test_post.py`** - Tests for post-processing (q-values, FDR)

### Integration Tests
- **`test_integration.py`** - End-to-end workflow tests
- **`test_cli.py`** - Command-line interface tests

### Specialized Tests
- **`test_edge_cases.py`** - Edge cases, error handling, and performance benchmarks

### Test Infrastructure
- **`conftest.py`** - Shared pytest fixtures
- **`utils.py`** - Test utilities and helper functions
- **`prepare_test_data.py`** - Script to generate test data from GEUVADIS examples

## Running Tests

### Install Test Dependencies

```bash
pip install -e .[test]
```

### Prepare Test Data

```bash
cd tests
python prepare_test_data.py
```

### Run All Tests

```bash
pytest tests/ -v
```

### Run Specific Test Categories

```bash
# Fast tests only (exclude slow and integration tests)
pytest tests/ -m "not slow and not integration"

# Integration tests only
pytest tests/ -m "integration"

# Performance benchmarks
pytest tests/ -m "benchmark"

# GPU-required tests (if GPU available)
pytest tests/ -m "gpu_required"
```

### Run Tests with Coverage

```bash
pip install pytest-cov
pytest tests/ --cov=tensorqtl --cov-report=html
```

## Test Data

The test suite uses two types of data:

1. **Synthetic Data**: Generated programmatically for unit tests
2. **GEUVADIS Subset**: Small subset of real data for integration tests

Test data is generated by `prepare_test_data.py` and stored in `tests/data/`:

- `synthetic_*` files: Small synthetic datasets for unit tests
- `test_*` files: Subsets of GEUVADIS data for integration tests

## Test Markers

Tests are organized using pytest markers:

- `@pytest.mark.slow`: Tests that take longer to run
- `@pytest.mark.integration`: End-to-end workflow tests
- `@pytest.mark.gpu_required`: Tests that require GPU
- `@pytest.mark.benchmark`: Performance benchmark tests

## CI/CD

Tests are automatically run via GitHub Actions on:
- Multiple Python versions (3.8, 3.9, 3.10, 3.11)
- CPU and GPU environments
- With coverage reporting

## Coverage Goals

- **Core functionality**: >90% coverage
- **I/O operations**: >85% coverage
- **Statistical methods**: >95% coverage
- **Edge cases**: Comprehensive coverage
- **Integration**: Key workflows validated

## Writing New Tests

When adding new functionality:

1. Add unit tests in the appropriate `test_*.py` file
2. Add integration tests if the feature affects workflows
3. Include edge cases and error conditions
4. Add performance tests for computationally intensive features
5. Update this README if adding new test categories

### Test Guidelines

- Use descriptive test names
- Test both success and failure cases
- Use appropriate fixtures from `conftest.py`
- Follow the existing test patterns
- Add docstrings for complex test logic
- Use parametrized tests for testing multiple conditions

## Debugging Tests

### Common Issues

1. **Import errors**: Ensure tensorqtl is properly installed (`pip install -e .`)
2. **Missing test data**: Run `prepare_test_data.py`
3. **GPU tests failing**: GPU tests skip automatically if CUDA unavailable
4. **Memory issues**: Reduce dataset sizes in performance tests

### Debugging Commands

```bash
# Run single test with detailed output
pytest tests/test_core.py::TestResidualizer::test_residualizer_init -v -s

# Run tests with debugger on failure
pytest tests/ --pdb

# Run tests with detailed traceback
pytest tests/ --tb=long
```

## Performance Testing

Performance tests help ensure tensorQTL scales appropriately:

- **Scaling tests**: Verify performance scales reasonably with data size
- **Memory tests**: Check memory usage stays within bounds
- **GPU vs CPU**: Compare performance when both available
- **Batch processing**: Validate batch processing improves performance

Run performance tests with:

```bash
pytest tests/ -m "benchmark" -v
```